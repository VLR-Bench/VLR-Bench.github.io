<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="VLR-Bench">
  <meta name="keywords" content="benchmark dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLR-Bench</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="./images/MLP_purple.png" alt="MLP Lab Logo" style="height:250px; margin-bottom:10px;">
            <h1 class="title is-1 publication-title">VLR-Bench: <span class="is-size-2"><span class="is-size-1">M</span>ultilingual <span class="is-size-1">B</span>enchmark <span class="is-size-1">D</span>ataset <span class="is-size-1">f</span>or <span class="is-size-1">V</span>ision-<span class="is-size-1">L</span>anguage <span class="is-size-1">R</span>etrieval <span class="is-size-1">A</span>ugmented <span class="is-size-1">G</span>eneration</span>
            </h1>
            <h3 class="title is-3 publication-title">Benchmark for Utility of Retrieved Documents</h3>
            <h5 class="subtitle is-5 publication-awards">COLING 2025</h5>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/lhsstn/" style="color:#f68946;font-weight:normal;">Hyeonseok Lim<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/faizman31/" style="color:#f68946;font-weight:normal;">Dongjae Shin<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/kreamsoup-SH" style="color:#f68946;font-weight:normal;">Seohyun Song</a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/kotmul" style="color:#f68946;font-weight:normal;">Inho Won</a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/mjkmain" style="color:#f68946;font-weight:normal;">Minjun Kim</a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Swalbak" style="color:#008AD7;font-weight:normal;;">Junghun Yuk</a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/aim-lab-hbnu/" style="color:#008AD7;font-weight:normal;">Haneol Jang</a>,
              </span>
              <span class="author-block">
                <a href="https://github.com/jujbob" style="color:#f68946;font-weight:normal;">KyungTae Lim</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> Seoul National University of Science and Technology</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Hanbat National University</span>
          
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.10151" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://aclanthology.org/2025.coling-main.411/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>COLING 2025</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/haotian-liu/LLaVA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/MLP-KTLim/VLR-IF" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>VLR-IF Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/MLP-KTLim/VLR-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>VLR-Bench Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>





  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.
              <ol type="1">
                <li><b>Multimodal Instruct Data</b>. <span style="font-size: 95%;">We present the first attempt to use <a href="https://openai.com/research/gpt-4">language-only GPT-4</a> to generate multimodal language-image instruction-following data. </span></li>
                <li><b>LLaVA Model</b>. <span style="font-size: 95%;">We introduce <it><b>LLaVA</b> (<b>L</b>arge <b>L</b>anguage-<b>a</b>nd-<b>V</b>ision <b>A</b>ssistant)</it>, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.</li>
                <li><b>Performance</b>. <span style="font-size: 95%;">Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset.
                  When fine-tuned on <a href="https://scienceqa.github.io/">Science QA</a>, the synergy of LLaVA and GPT-4  achieves a new state-of-the-art accuracy of 92.53%.</li>
                <li><b>Open-source</b>. <span style="font-size: 95%;">We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.</li>
              </ol>  
           </p>
  
          </div>
        </div>
      </div>
        
    </div>
  </section>


  
<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/9422/9422946.png"> VLR-Bench Dataset</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <p>
          We manually selected 150 images from the BOK-VQA benchmark dataset and an additional 150 images reflecting the cultural elements of each language from Wikimedia Commons. Subsequently, we utilized the GPT-4o model with few-shot samples to create the VLR-Bench benchmark dataset. VLR-Bench is a parallel corpus consisting of English, Chinese, and Korean, with a total of 300 samples. The VLR-Bench dataset is available at the following link: 
          <a href="https://huggingface.co/datasets/MLP-KTLim/VLR-Bench">[HuggingFace Dataset]</a>.
        </p>
        <p>
          The following figures are examples from VLR-BENCH. Each example consists of a question, an answer, keywords, and passages. The “gold passage”, which contains the information necessary to answer the question, is highlighted in yellow.
        </p>
      </div>

      <!-- 세로 배치 -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-half" style="display: flex; flex-direction: column; align-items: center; gap: 5px;">
          <figure style="text-align: center; width: 80%;">
            <img id="teaser" src="./images/vlr_bench_ex0.png" width="100%" height="400px">
            <figcaption>Examples of the created VLR-Bench data. (English culture)</figcaption>
          </figure>
          <figure style="text-align: center; width: 80%;">
            <img id="teaser" src="./images/vlr_bench_ex1.png" width="100%" height="400px">
            <figcaption>Examples of the created VLR-Bench data. (commonsense knowledge)</figcaption>
          </figure>
        </div>

        <!-- 새 내용 추가 -->
        <div class="column is-half">
          <figure style="text-align: center; width: 80%; margin: 0 auto;">
            <img id="process" src="./images/vlr_bench_process.png" width="100%" height="700px">
            <figcaption>Overview of the VLR-BENCH dataset construction process.</figcaption>
          </figure>
        </div>
        </div>
      </div>
        

    </div>
  </div>


</section>
 

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/4149/4149678.png"> VLR-IF Dataset</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          Before conducting evaluations with our VLR-Bench, we built the VLR-IF (Instruction Following) dataset to ensure the model can extract accurate information from documents. A total of 9,000 images were randomly selected from the COCO image dataset. Following a process similar to the construction of VLR-Bench, we provided few-shot examples to GPT-4o to generate "valid passages." Subsequently, we used valid passages from other image samples as "invalid passages" and combined them to create a parallel corpus of 32,000 entries spanning English, Chinese, and Korean. This dataset is available at the following link:
          <a href="https://huggingface.co/datasets/MLP-KTLim/VLR-IF">[HuggingFace Dataset]</a>.
          Ultimately, we constructed a total of 32,000 datasets by combining valid and invalid passages in the following manner: {V}, {I}, {V, I}, {V, I, I}.
          <li><b>{V}</b>: <span style="font-size: 95%;">Only the valid passage, 9,000 datasets.</span></li>
          <li><b>{I}</b>: <span style="font-size: 95%;">Only one invalid passage, 5,000 datasets. In this case, the training data was constructed to output "Insufficient search results found, making inference impossible" when encountering such instances. 
          <li><b>{V, I}</b>: <span style="font-size: 95%;">One valid and one invalid passage, 9,000 datasets.
          <li><b>{V, I, I}</b>: <span style="font-size: 95%;">One valid and two invalid passages, 9,000 datasets.
        </p>
      </div>
      <centering>
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">  
            <figure style="text-align: center; width: 60%; margin-right: 10px;">
              <img id="teaser" src="./images/vlr_if_ex0.png" width="100%" height="400px">  
              <figcaption>Examples of the created VLR-IF data.</figcaption>
            </figure>
            <figure style="text-align: center; width: 60%;">
              <embed id="teaser" src="./images/vlr_if_process.png" width="100%" height="300px">
              <figcaption>The process of constructing the VLR-IF dataset</figcaption>
            </figure>
          </div>
        </div>
      
      </centering>           
    </div>
  </div>


</section>
  


<!-- <section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn-icons-png.flaticon.com/512/9912/9912341.png"> Evaluation</h2>
    </div>
  </div>

  <img src="./images/result_table.png" alt="VLR Bench result Table" style="height:250px; margin-bottom:10px;">

<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><img id="painting_icon" width="4%" src="https://cdn-icons-png.flaticon.com/512/1698/1698535.png"> <span style="font-size: 100%;">Visual Chat:</span> Towards building multimodal GPT-4 level chatbot  </h2>
      
      <div>
        <a href="https://plotly.com/~lichunyuan24/5/?share_key=d78QObaCAYCIy8PJpe3gd1" target="_blank" title="llava_gpt4_pie" style="display: block; text-align: center;">  <img id="painting_icon" width="90%" src="images/pie_llava_gpt4.png"> </a>

    </div>

    <p style="font-family:Times New Roman"><b>An evaluation dataset with 30 unseen images is constructed: each image is assocaited with three types of instructions: conversation, detailed description and complex reasoning. This leads to 90 new language-image instructions, on which we test LLaVA and GPT-4, and use GPT-4 to rate their responses from score 1 to 10. The summed score and relative score per type is reported. Overall, LLaVA achieves 85.1% relative score compared with GPT-4, indicating the effectinvess of the proposed self-instruct method in multimodal settings</b>               
    </div>
  </div>

  <!-- Grounedtext2img. -->
  <!-- <div class="columns is-centered"> -->
    <!-- <div class="column is-full-width"> -->
      <!-- <h2 class="title is-4"> <img id="painting_icon" width="3%" src="https://scienceqa.github.io/img/logo.png"><span style="font-size: 100%;"> Science QA:</span> New SoTA with the synergy of LLaVA with GPT-4</h2> -->
      
      <!-- <div> -->
        <!-- <a href="https://plotly.com/~lichunyuan24/1/?share_key=v4opE3TJpxqQ08RYsDD4iv" target="_blank" title="Plot 1" style="display: block; text-align: center;"><img id="painting_icon" width="65%" src="images/bar_llava_gpt4_scienceqa.png"></a> -->
        <!-- <script data-plotly="lichunyuan24:1" sharekey-plotly="v4opE3TJpxqQ08RYsDD4iv" src="https://plotly.com/embed.js" async></script> -->
    <!-- </div> -->
        <!-- <p style="font-family:Times New Roman"><b>LLaVA alones achieve 90.92%. We use the text-only GPT-4 as the judge, to predict the final answer based on its own previous answers and the LLaVA answers. This "GPT-4 as judge" scheme yields a new SOTA 92.53%.</b> -->
              
    <!-- </div> -->
  <!-- </div> -->


</section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{lim2024vlr,
  title={VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation},
  author={Lim, Hyeonseok and Shin, Dongjae and Song, Seohyun and Won, Inho and Kim, Minjun and Yuk, Junghun and Jang, Haneol and Lim, KyungTae},
  journal={arXiv preprint arXiv:2412.10151},
  year={2024}
}

@inproceedings{lim-etal-2025-vlr,
    title = "{VLR}-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
    author = "Lim, Hyeonseok  and
      Shin, Dongjae  and
      Song, Seohyun  and
      Won, Inho  and
      Kim, Minjun  and
      Yuk, Junghun  and
      Jang, Haneol  and
      Lim, KyungTae",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.411/",
    pages = "6150--6168",
    abstract = "We propose the VLR-Bench, a visual question answering (VQA) benchmark for evaluating vision language models (VLMs) based on retrieval augmented generation (RAG). Unlike existing evaluation datasets for external knowledge-based VQA, the proposed VLR-Bench includes five input passages. This allows testing of the ability to determine which passage is useful for answering a given query, a capability lacking in previous research. In this context, we constructed a dataset of 32,000 automatically generated instruction-following examples, which we denote as VLR-IF. This dataset is specifically designed to enhance the RAG capabilities of VLMs by enabling them to learn how to generate appropriate answers based on input passages. We evaluated the validity of the proposed benchmark and training data and verified its performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3 model. The proposed VLR-Bench and VLR-IF datasets are publicly available online."
}
  </code></pre>
    </div>
  </section>
  
  <!-- <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.  We thank the LLaMA team for giving us access to their models, and open-source projects, including Alpaca and Vicuna.
      </p>

      <p>
<b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
</p>

      <p>
      <a href='https://github.com/Computer-Vision-in-the-Wild/'><img id="painting_icon" width="10%" src="https://avatars.githubusercontent.com/u/97258247?s=200&v=4"></a> 
      Related Links: 
      <a href='https://react-vl.github.io/'>[REACT]</a>  
      <a href='https://gligen.github.io/'>[GLIGEN]</a> 
      <a href='https://github.com/Computer-Vision-in-the-Wild/'>[Computer Vision in the Wild (CVinW)]</a> 
      <a href='https://instruction-tuning-with-gpt-4.github.io/'>[Insutrction Tuning with GPT-4]</a>      
      </p>    
    </div>
  </section> -->


</body>

</html>
